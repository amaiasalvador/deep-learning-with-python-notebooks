{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char RNN: character-level language modelling\n",
    "\n",
    "We will work with the CharRNN model for character-level language modelling. In a nutshell, this model aims at modelling the probability distribution of the next character given the sequence of previous characters. These models can be trained using any text source, since the text itself is the supervision signal for the task. Please see [this blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) for more examples.\n",
    "\n",
    "The code in this tutorial is based on [this implementation](https://github.com/crazydonkey200/tensorflow-char-rnn) by *crazydonkey200*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "from util.text_tools import *\n",
    "\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "We will create the model containing an embedding layer, an RNN and a fully connected layer. It will be defined in a class with three methods:\n",
    "\n",
    "- `__init__` instantiates the class and is in charge of creating the graph. It does not perform any computation.\n",
    "- `run_epoch` runs the model once on all the data. We will use it for training the weights, although it can be used for evaluation on a validation set as well by setting `is_training=False`.\n",
    "- `sample_seq` takes an input text and samples a sequence of characters from the model. Used for inference on user generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(object):\n",
    "    \"\"\"Character RNN model.\"\"\"\n",
    "  \n",
    "    def __init__(self, is_training, batch_size, num_unrollings, vocab_size, hidden_size=128, max_grad_norm=0.2, \n",
    "                 embedding_size=15, learning_rate=0.01):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_unrollings = num_unrollings\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.embedding_size = embedding_size\n",
    "        self.input_size = embedding_size\n",
    "\n",
    "        # Placeholder to feed in input and targets/labels data\n",
    "        self.input_data = tf.placeholder(tf.int64, [self.batch_size, self.num_unrollings], name='inputs')\n",
    "        self.targets = tf.placeholder(tf.int64, [self.batch_size, self.num_unrollings], name='targets')\n",
    "\n",
    "        # Embeddings layer\n",
    "        with tf.name_scope('embedding_layer'):\n",
    "            self.embedding = tf.get_variable('embedding', [self.vocab_size, self.embedding_size])\n",
    "            inputs = tf.nn.embedding_lookup(self.embedding, self.input_data)\n",
    "\n",
    "        # Create multilayer GRU cell.\n",
    "        cell = tf.contrib.rnn.GRUCell(self.hidden_size, reuse=tf.get_variable_scope().reuse)\n",
    "\n",
    "        # The initial state for the RNN is the null vector\n",
    "        with tf.name_scope('initial_state'):\n",
    "            self.zero_state = cell.zero_state(self.batch_size, tf.float32)\n",
    "            \n",
    "        # Create a placeholder to propagate the RNN state between batches\n",
    "        self.initial_state = tf.placeholder(tf.float32, self.zero_state.get_shape())        \n",
    "        \n",
    "        # Create the graph for the RNN by unrolling it in time\n",
    "        rnn_outputs, self.final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=self.initial_state, \n",
    "                                                    dtype=tf.float32)\n",
    "\n",
    "        # Classification layer on top of the RNN\n",
    "        with tf.variable_scope('softmax') as sm_vs:\n",
    "            self.logits = layers.linear(inputs=rnn_outputs, num_outputs=vocab_size)\n",
    "            self.probs = tf.nn.softmax(self.logits)\n",
    "\n",
    "        # Compute mean cross entropy loss for each output\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.targets)\n",
    "            self.mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "        # Track metrics (cross-entropy loss and perplexity) using TensorBoard\n",
    "        with tf.name_scope('loss_monitor'):\n",
    "            count = tf.Variable(1.0, name='count')\n",
    "            sum_mean_loss = tf.Variable(1.0, name='sum_mean_loss')\n",
    "\n",
    "            self.reset_loss_monitor = tf.group(sum_mean_loss.assign(0.0), count.assign(0.0), name='reset_loss_monitor')\n",
    "            self.update_loss_monitor = tf.group(sum_mean_loss.assign(sum_mean_loss + self.mean_loss),\n",
    "                                                count.assign(count + 1), name='update_loss_monitor')\n",
    "            with tf.control_dependencies([self.update_loss_monitor]):\n",
    "                self.average_loss = sum_mean_loss / count\n",
    "                self.ppl = tf.exp(self.average_loss)\n",
    "        average_loss_summary = tf.summary.scalar(\"average_loss\", self.average_loss)\n",
    "        ppl_summary = tf.summary.scalar(\"perplexity\", self.ppl)\n",
    "        self.summaries = tf.summary.merge([average_loss_summary, ppl_summary], name='loss_monitor')\n",
    "\n",
    "        # Track number of SGD steps\n",
    "        self.global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "        # Create training op: (1) compute gradients, (2) clip their norm, and (3) update weights\n",
    "        if is_training:\n",
    "            optimizer = tf.train.AdamOptimizer(tf.constant(learning_rate))\n",
    "            trainable_variables = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.mean_loss, trainable_variables), self.max_grad_norm)\n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, trainable_variables), global_step=self.global_step)\n",
    "\n",
    "            \n",
    "    def run_epoch(self, session, data_size, batch_generator, is_training, summary_writer=None, epoch_number=None):\n",
    "        \"\"\"Runs the model on the given data for one full pass, (optionally) training the model weigths.\"\"\"\n",
    "        epoch_size = data_size // (self.batch_size * self.num_unrollings)\n",
    "        if data_size % (self.batch_size * self.num_unrollings) != 0:\n",
    "            epoch_size += 1\n",
    "\n",
    "        if is_training:\n",
    "            extra_op = self.train_op\n",
    "        else:\n",
    "            extra_op = tf.no_op()\n",
    "\n",
    "        # Prepare initial state and reset the average loss computation.\n",
    "        state = session.run(self.zero_state)\n",
    "        self.reset_loss_monitor.run()\n",
    "        start_time = time.time()\n",
    "        for step in range(epoch_size):\n",
    "            # Generate the batch and use [:-1] as inputs and [1:] as targets.\n",
    "            data = batch_generator.next()\n",
    "            inputs = np.array(data[:-1]).transpose()\n",
    "            targets = np.array(data[1:]).transpose()\n",
    "\n",
    "            ops = [self.average_loss, self.final_state, extra_op, self.summaries]\n",
    "            feed_dict = {self.input_data: inputs, self.targets: targets, self.initial_state: state}\n",
    "            average_loss, state, _, summary_str = session.run(ops, feed_dict)\n",
    "\n",
    "            if summary_writer and (step+1) % 100 == 0:\n",
    "                summary_writer.add_summary(summary_str, session.run(self.global_step))\n",
    "            \n",
    "            ppl = np.exp(average_loss)\n",
    "            if (step+1) % 500 == 0:\n",
    "                if epoch_number is not None:\n",
    "                    print(\"[Epoch %d] \" % epoch_number, end=\"\")\n",
    "                print(\"%.1f%%, step:%d, perplexity: %.3f, speed: %.0f words per sec\" % \n",
    "                      ((step + 1) * 1.0 / epoch_size * 100, step, ppl,\n",
    "                      (step + 1) * self.batch_size * self.num_unrollings /\n",
    "                      (time.time() - start_time)))\n",
    "\n",
    "        if epoch_number is not None:\n",
    "            print(\"[Epoch %d] \" % epoch_number, end=\"\")\n",
    "        print(\"Perplexity: %.3f, speed: %.0f words per sec\\n\" %\n",
    "              (ppl, (step + 1) * self.batch_size * self.num_unrollings / (time.time() - start_time)))\n",
    "        return ppl, summary_str\n",
    "\n",
    "    def sample_seq(self, session, length, start_text, vocab_index_dict,\n",
    "                 index_vocab_dict, temperature=1.0, deterministic=True):\n",
    "\n",
    "        state = session.run(self.zero_state)\n",
    "\n",
    "        # use start_text to warm up the RNN.\n",
    "        if start_text is not None and len(start_text) > 0:\n",
    "            seq = list(start_text)\n",
    "            for char in start_text[:-1]:\n",
    "                x = np.array([[char2id(char, vocab_index_dict)]])\n",
    "                state = session.run(self.final_state,\n",
    "                                    {self.input_data: x,\n",
    "                                     self.initial_state: state})\n",
    "            x = np.array([[char2id(start_text[-1], vocab_index_dict)]])\n",
    "        else:\n",
    "            vocab_size = len(vocab_index_dict.keys())\n",
    "            x = np.array([[np.random.randint(0, vocab_size)]])\n",
    "            seq = []\n",
    "\n",
    "        for i in range(length):\n",
    "            state, logits = session.run([self.final_state,\n",
    "                                   self.logits],\n",
    "                                  {self.input_data: x,\n",
    "                                   self.initial_state: state})\n",
    "            unnormalized_probs = np.exp((logits - np.max(logits)) / temperature)\n",
    "            probs = unnormalized_probs / np.sum(unnormalized_probs)\n",
    "\n",
    "            if deterministic:\n",
    "                sample = np.argmax(probs[0])\n",
    "            else:\n",
    "                sample = np.random.choice(self.vocab_size, 1, p=probs[0][0])[0]\n",
    "                \n",
    "            seq.append(id2char(sample, index_vocab_dict))\n",
    "            x = np.array([[sample]])\n",
    "        return ''.join(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_unrollings = 30\n",
    "num_epochs = 5\n",
    "\n",
    "text_file = 'data/tiny_shakespeare.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(text):\n",
    "    unique_chars = list(set(text))\n",
    "    vocab_size = len(unique_chars)\n",
    "    vocab_index_dict = {}\n",
    "    index_vocab_dict = {}\n",
    "    for i, char in enumerate(unique_chars):\n",
    "        vocab_index_dict[char] = i\n",
    "        index_vocab_dict[i] = char\n",
    "    return vocab_index_dict, index_vocab_dict, vocab_size\n",
    "\n",
    "with open(text_file, 'r', encoding='utf-8') as f:\n",
    "    train_text = f.read()\n",
    "vocab_index_dict, index_vocab_dict, vocab_size = create_vocab(train_text)\n",
    "print(\"Vocabulary (\", vocab_size, \"elements ): \", sorted(vocab_index_dict.keys()))\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings, vocab_size, vocab_index_dict, index_vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset graph in case we already created one and want to change hyperparameters\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Training graph\n",
    "with tf.name_scope('training'):\n",
    "    train_model = CharRNN(is_training=True, batch_size=batch_size, num_unrollings=num_unrollings, \n",
    "                          vocab_size=vocab_size)\n",
    "\n",
    "# Both models share weights\n",
    "tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "# Evaluation graph\n",
    "with tf.name_scope('evaluation'):\n",
    "    test_model = CharRNN(is_training=False, batch_size=1, num_unrollings=1, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Initialize variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# TensorBoard: create an empty logdir and the writer\n",
    "logdir = 'tensorboard_logs/char_rnn'\n",
    "if tf.gfile.Exists(logdir):\n",
    "    tf.gfile.DeleteRecursively(logdir)\n",
    "if not tf.gfile.Exists(logdir):\n",
    "    tf.gfile.MakeDirs(logdir)\n",
    "summary_writer = tf.summary.FileWriter(logdir)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    train_model.run_epoch(sess, len(train_text), train_batches, is_training=True, \n",
    "                          summary_writer=summary_writer, epoch_number=epoch+1)\n",
    "summary_writer.flush()\n",
    "summary_writer.close()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample sentences from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 150\n",
    "deterministic = False\n",
    "softmax_temperature = 0.5\n",
    "start_text = \"The meaning of life is \"\n",
    "\n",
    "sample = test_model.sample_seq(sess, num_samples, start_text, vocab_index_dict, index_vocab_dict, \n",
    "                               softmax_temperature, deterministic)\n",
    "print('Sampled text is:\\n%s' % sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible extensions\n",
    "\n",
    "- Reduce the learning rate when the loss plateaus.\n",
    "- Split data into train/val/test for a better experimental setup. Tune hyperparameters to maximize performance on the validation set.\n",
    "- Add more capacity to the model: more layers (check `tf.contrib.rnn.MultiRNNCell`) and more units per layer.\n",
    "- If the model capacity is increased, regularization may be needed to avoid overfitting. For instance, dropout between RNN layers (check `tf.contrib.rnn.DropoutWrapper`).\n",
    "- Train the model on your own data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
