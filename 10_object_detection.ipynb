{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session we will see how to build a simple ConvNet for a toy object detection problem. We train the network from scratch and we will see the particularities of the problem of object detection. Finally, we will use a state of the art object detection network on some real-world images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is adapted from the tutorial:\n",
    "\n",
    "[Object detection with neural networks — a simple tutorial using keras](https://medium.com/towards-data-science/object-detection-with-neural-networks-a4e2c46b4491)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will look at object detection — finding out which objects are in an image. For example, imagine a self-driving car that needs to detect other cars on the road. There are lots of complicated algorithms for object detection. They often require huge datasets, very deep convolutional networks and long training times. To make this tutorial easy to follow along, we’ll apply two simplifications: 1) We don’t use real photographs, but images with abstract geometric shapes. This allows us to bootstrap the image data and use simpler neural networks. 2) We predict a fixed number of objects in each image. This makes the entire algorithm a lot, lot easier (it’s actually surprisingly simple besides a few tricks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deal with images with simple objects of different shapes. For simplicity, all images will have the same size, are black&white and have two objects. We will create the dataset ourselves with these lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs = 50000\n",
    "\n",
    "img_size = 16\n",
    "min_rect_size = 3\n",
    "max_rect_size = 8\n",
    "num_objects = 2\n",
    "\n",
    "bboxes = np.zeros((num_imgs, num_objects, 4))\n",
    "imgs = np.zeros((num_imgs, img_size, img_size))\n",
    "shapes = np.zeros((num_imgs, num_objects, 1))\n",
    "\n",
    "for i_img in range(num_imgs):\n",
    "    for i_object in range(num_objects):\n",
    "        # half of the objects will be triangles, the other half are squares\n",
    "        if np.random.choice([True, False]):\n",
    "            width, height = np.random.randint(min_rect_size, max_rect_size, size=2)\n",
    "            x = np.random.randint(0, img_size - width)\n",
    "            y = np.random.randint(0, img_size - height)\n",
    "            imgs[i_img, x:x+width, y:y+height] = 1.\n",
    "            bboxes[i_img, i_object] = [x, y, width, height]\n",
    "            shapes[i_img, i_object] = [0]\n",
    "        else:\n",
    "            size = np.random.randint(min_rect_size, max_rect_size)\n",
    "            x, y = np.random.randint(0, img_size - size, size=2)\n",
    "            mask = np.tril_indices(size)\n",
    "            imgs[i_img, x + mask[0], y + mask[1]] = 1.\n",
    "            bboxes[i_img, i_object] = [x, y, size, size]\n",
    "            shapes[i_img, i_object] = [1]\n",
    "            \n",
    "imgs.shape, bboxes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a dataset of 50k images. Let's look at some of the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "plt.imshow(imgs[i].T, cmap='Greys', interpolation='none', \n",
    "           origin='lower', extent=[0, img_size, 0, img_size])\n",
    "for bbox, shape in zip(bboxes[i], shapes[i]):\n",
    "    plt.gca().add_patch(matplotlib.patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], \n",
    "                                                     ec='r' if shape[0] == 0 else 'y', fc='none'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pre-process the images by subtracting the mean and dividing by the std of pixel intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (imgs - np.mean(imgs)) / np.std(imgs)\n",
    "X.shape, np.mean(X), np.std(X)\n",
    "X = X.reshape((X.shape[0],X.shape[1],X.shape[2],1))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box coordinates are also pre-processed so that instead of predicting raw pixel coordinates `(w,h)`, we predict `(w/W, w/H)`. We can decode these values easily at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate([bboxes / img_size, shapes], axis=-1).reshape(num_imgs, -1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y` is a vector of 10 positions (5 for each object). Out of the 5 values, 4 are the pixel coordinates, and the 4th is the one indicating wether the object is a triangle or a square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split our data into train/test splits. We will use 80% of the data for training, and the rest for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_train = int(0.6 * num_imgs)\n",
    "i_val = int(0.7 * num_imgs)\n",
    "\n",
    "train_X = X[:i_train]\n",
    "val_X = X[i_train:i_val]\n",
    "test_X = X[i_val:]\n",
    "train_y = y[:i_train]\n",
    "val_y = y[i_train:i_val]\n",
    "test_y = y[i_val:]\n",
    "test_imgs = imgs[i_val:]\n",
    "test_bboxes = bboxes[i_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a simple model with a single hidden Dense layer with 256 dimensions, and a Dense layer of 10 dimensions. We will use MSE as our loss function to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Conv2D(filters=64,kernel_size=3, input_shape=(X.shape[1:]),activation='relu'), \n",
    "        Conv2D(filters=32,kernel_size=3,activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(y.shape[-1])\n",
    "    ])\n",
    "model.compile('adadelta', 'mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's go ahead and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "history = model.fit(train_X,train_y,batch_size=512,epochs=n_epochs,validation_data=(val_X,val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some results. We can use `model.predict()` to get the output of our model for all our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(test_X)\n",
    "pred_y = pred_y.reshape(len(pred_y), num_objects, -1)\n",
    "pred_bboxes = pred_y[..., :4] * img_size\n",
    "pred_shapes = pred_y[..., 4:5]\n",
    "pred_bboxes.shape, pred_shapes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "for i_subplot in range(1, 9):\n",
    "    plt.subplot(2, 4, i_subplot)\n",
    "    i = np.random.randint(len(test_X))\n",
    "    plt.imshow(test_imgs[i].T, cmap='Greys', interpolation='none', origin='lower', \n",
    "               extent=[0, img_size, 0, img_size])\n",
    "    for pred_bbox, exp_bbox, pred_shape in zip(pred_bboxes[i], test_bboxes[i], pred_shapes[i]):\n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((pred_bbox[0], pred_bbox[1]), pred_bbox[2], pred_bbox[3], \n",
    "                                                         ec='r' if pred_shape[0] <= 0.5 else 'y', fc='none'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both bounding boxes seem to be in the middle of the rectangles. What happened? Imagine the following situation: We train our network on the leftmost image in the plot above. Let’s say that the expected bounding box of the left rectangle is at position 1 in the target vector (x1, y1, w1, h1), and the expected bounding box of the right rectangle is at position 2 in the vector (x2, y2, w2, h2). Apparently, our optimizer will change the parameters of the network so that the first predictor moves to the left, and the second predictor moves to the right. Imagine now that a bit later we come across a similar image, but this time the positions in the target vector are swapped (i.e. left rectangle at position 2, right rectangle at position 1). Now, our optimizer will pull predictor 1 to the right and predictor 2 to the left — exactly the opposite of the previous update step! In effect, the predicted bounding boxes stay in the center. And as we have a huge dataset (40k images), there will be quite a lot of such “duplicates”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to “assign” each predicted bounding box to a rectangle during training. Then, the predictors can learn to specialize on certain locations and/or shapes of rectangles. In order to do this, we process the target vectors after every epoch: For each training image, we calculate the mean squared error between the prediction and the target A) for the current order of bounding boxes in the target vector (i.e. x1, y1, w1, h1, x2, y2, w2, h2) and B) if the bounding boxes in the target vector are flipped (i.e. x2, y2, w2, h2, x1, y1, w1, h1). If the MSE of A is greater than B, we leave the target vector as is; if the MSE of B is greater than A, we flip the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write a function that permutes the ground truth labels based on the obtained mean square error. This function uses the library `munkres` to compute the minimum cost assignment given the matrix of costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from munkres import Munkres\n",
    "\n",
    "def flip_targets(pred_ys,ys,m):\n",
    "\n",
    "    for sample, (pred_y,y) in enumerate(zip(pred_ys,ys)):\n",
    "        pred_y = pred_y.reshape(num_objects, -1)\n",
    "        y = y.reshape(num_objects, -1)\n",
    "        \n",
    "        pred_bboxes = pred_y[:, :4]\n",
    "        y_bboxes = y[:, :4]\n",
    "        costs = np.zeros((num_objects,num_objects))\n",
    "        for i, y_bbox in enumerate(y_bboxes):\n",
    "            for j, pred_bbox in enumerate(pred_bboxes):\n",
    "                costs[i, j] += np.mean(np.square(y_bbox - pred_bbox))\n",
    "                \n",
    "        # matrix of assignments\n",
    "        idxs = np.array(m.compute(costs))[:,1]\n",
    "        y_flipped = y[idxs]\n",
    "        ys[sample] = y_flipped.flatten()\n",
    "        \n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Conv2D(filters=64,kernel_size=3, input_shape=(X.shape[1:]),activation='relu'), \n",
    "        Conv2D(filters=32,kernel_size=3,activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(y.shape[-1])\n",
    "    ])\n",
    "model.compile('adadelta', 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train with an external loop this time. This will allow us to flip the ground truth at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = Munkres()\n",
    "for epoch in range(n_epochs):\n",
    "    print ('Epoch', epoch)\n",
    "    model.fit(train_X,train_y,batch_size=512,epochs=1,validation_data=(val_X,val_y))\n",
    "    pred_y_train = model.predict(train_X)\n",
    "    pred_y_val = model.predict(val_X)\n",
    "    \n",
    "    train_y = flip_targets(pred_y_train,train_y,m)\n",
    "    # For speed, we will not flip the validation ground truth. This means that validation will not be informative!\n",
    "    #val_y = flip_targets(pred_y_val,val_y,m)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's decode some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(test_X)\n",
    "pred_y = pred_y.reshape(len(pred_y), num_objects, -1)\n",
    "pred_bboxes = pred_y[..., :4] * img_size\n",
    "pred_shapes = pred_y[..., 4:5]\n",
    "pred_bboxes.shape, pred_shapes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "for i_subplot in range(1, 9):\n",
    "    plt.subplot(2, 4, i_subplot)\n",
    "    i = np.random.randint(len(test_X))\n",
    "    plt.imshow(test_imgs[i].T, cmap='Greys', interpolation='none', \n",
    "               origin='lower', extent=[0, img_size, 0, img_size])\n",
    "    for pred_bbox, exp_bbox, pred_shape in zip(pred_bboxes[i], test_bboxes[i], pred_shapes[i]):\n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((pred_bbox[0], pred_bbox[1]), pred_bbox[2], pred_bbox[3], \n",
    "                                                         ec='r' if pred_shape[0] <= 0.5 else 'y', fc='none'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It still is not perfect (we should probably train longer), but you can now see how the boxes are getting aligned with the objects in the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get real..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recognizing shapes is a cool and easy example, but obviously it’s not what you want to do in the real world (there aren’t that many abstract 2D shapes in nature, unfortunately). Also, our algorithm can only predict a fixed number of bounding boxes per image. In the real world, however, you have diverse scenarios: A small side road may have no cars on it, but as soon as you drive on the highway, you have to recognize hundreds of cars at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For time constraints, and given the complexity of the architectures and training procedures for real-world object detection problems, we will only use a pretrained model to test it on some images.\n",
    "\n",
    "We will use the architecture from:\n",
    "Redmon & Farhadi. [YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242). CVPR 2017\n",
    "\n",
    "Which is officially implemented in a framework called Darknet. Here we will use an unofficial [port to keras](https://github.com/experiencor/basic-yolo-keras) to load and test the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has been trained with Pascal VOC data, and therefore can find objects from these 20 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'util/yolo/')\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os, cv2\n",
    "from util.yolo.model import *\n",
    "from util.yolo.utils import interpret_netout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('data/models/yolo-voc_weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to test the model on some images. Here we load and preprocess one of them to be fed into the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('data/images/bus_creative_commons.jpg')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "input_image = cv2.resize(image, (416, 416))\n",
    "input_image = input_image / 255.\n",
    "input_image = input_image[:,:,::-1]\n",
    "input_image = np.expand_dims(input_image, 0)\n",
    "\n",
    "netout = model.predict(input_image)\n",
    "\n",
    "#print netout\n",
    "image = interpret_netout(image, netout[0])\n",
    "plt.imshow(image[:,:,::-1]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
