{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is adapted from the tutorial:\n",
    "\n",
    "[Object detection with neural networks — a simple tutorial using keras](https://medium.com/towards-data-science/object-detection-with-neural-networks-a4e2c46b4491)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will look at object detection — finding out which objects are in an image. For example, imagine a self-driving car that needs to detect other cars on the road. There are lots of complicated algorithms for object detection. They often require huge datasets, very deep convolutional networks and long training times. To make this tutorial easy to follow along, we’ll apply two simplifications: 1) We don’t use real photographs, but images with abstract geometric shapes. This allows us to bootstrap the image data and use simpler neural networks. 2) We predict a fixed number of objects in each image. This makes the entire algorithm a lot, lot easier (it’s actually surprisingly simple besides a few tricks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deal with images with simple objects of different shapes. For simplicity, all images will have the same size, are black&white and have two objects. We will create the dataset ourselves with these lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 16, 16), (50000, 2, 4))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_imgs = 50000\n",
    "\n",
    "img_size = 16\n",
    "min_rect_size = 3\n",
    "max_rect_size = 8\n",
    "num_objects = 2\n",
    "\n",
    "bboxes = np.zeros((num_imgs, num_objects, 4))\n",
    "imgs = np.zeros((num_imgs, img_size, img_size))\n",
    "shapes = np.zeros((num_imgs, num_objects, 1))\n",
    "\n",
    "for i_img in range(num_imgs):\n",
    "    for i_object in range(num_objects):\n",
    "        # half of the objects will be triangles, the other half are squares\n",
    "        if np.random.choice([True, False]):\n",
    "            width, height = np.random.randint(min_rect_size, max_rect_size, size=2)\n",
    "            x = np.random.randint(0, img_size - width)\n",
    "            y = np.random.randint(0, img_size - height)\n",
    "            imgs[i_img, x:x+width, y:y+height] = 1.\n",
    "            bboxes[i_img, i_object] = [x, y, width, height]\n",
    "            shapes[i_img, i_object] = [0]\n",
    "        else:\n",
    "            size = np.random.randint(min_rect_size, max_rect_size)\n",
    "            x, y = np.random.randint(0, img_size - size, size=2)\n",
    "            mask = np.tril_indices(size)\n",
    "            imgs[i_img, x + mask[0], y + mask[1]] = 1.\n",
    "            bboxes[i_img, i_object] = [x, y, size, size]\n",
    "            shapes[i_img, i_object] = [1]\n",
    "            \n",
    "imgs.shape, bboxes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a dataset of 50k images. Let's look at some of the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLzXK8EuNMvxSowy/1CjDLzXq/wGHTl8tp10GFQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2eafa8f7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=1\n",
    "plt.imshow(imgs[i].T, cmap='Greys', interpolation='none', \n",
    "           origin='lower', extent=[0, img_size, 0, img_size])\n",
    "for bbox, shape in zip(bboxes[i], shapes[i]):\n",
    "    plt.gca().add_patch(matplotlib.patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], \n",
    "                                                     ec='r' if shape[0] == 0 else 'y', fc='none'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pre-process the images by subtracting the mean and dividing by the std of pixel intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 16, 16, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = (imgs - np.mean(imgs)) / np.std(imgs)\n",
    "X.shape, np.mean(X), np.std(X)\n",
    "X = X.reshape((X.shape[0],X.shape[1],X.shape[2],1))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box coordinates are also pre-processed so that instead of predicting raw pixel coordinates `(w,h)`, we predict `(w/W, w/H)`. We can decode these values easily at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.concatenate([bboxes / img_size, shapes], axis=-1).reshape(num_imgs, -1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y` is a vector of 10 positions (5 for each object). Out of the 5 values, 4 are the pixel coordinates, and the 4th is the one indicating wether the object is a triangle or a square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.125 ,  0.375 ,  0.3125,  0.4375,  0.    ,  0.5625,  0.375 ,\n",
       "        0.375 ,  0.375 ,  1.    ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split our data into train/test splits. We will use 80% of the data for training, and the rest for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_train = int(0.6 * num_imgs)\n",
    "i_val = int(0.7 * num_imgs)\n",
    "\n",
    "train_X = X[:i_train]\n",
    "val_X = X[i_train:i_val]\n",
    "test_X = X[i_val:]\n",
    "train_y = y[:i_train]\n",
    "val_y = y[i_train:i_val]\n",
    "test_y = y[i_val:]\n",
    "test_imgs = imgs[i_val:]\n",
    "test_bboxes = bboxes[i_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a simple model with a single hidden Dense layer with 256 dimensions, and a Dense layer of 10 dimensions. We will use MSE as our loss function to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 32)        18464     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                46090     \n",
      "=================================================================\n",
      "Total params: 65,194\n",
      "Trainable params: 65,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "        Conv2D(filters=64,kernel_size=3, input_shape=(X.shape[1:]),activation='relu'), \n",
    "        Conv2D(filters=32,kernel_size=3,activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(y.shape[-1])\n",
    "    ])\n",
    "model.compile('adadelta', 'mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's go ahead and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "history = model.fit(train_X,train_y,batch_size=32,epochs=n_epochs,validation_data=(val_X,val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some results. We can use `model.predict()` to get the output of our model for all our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(test_X)\n",
    "pred_y = pred_y.reshape(len(pred_y), num_objects, -1)\n",
    "pred_bboxes = pred_y[..., :4] * img_size\n",
    "pred_shapes = pred_y[..., 4:5]\n",
    "pred_bboxes.shape, pred_shapes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "for i_subplot in range(1, 9):\n",
    "    plt.subplot(2, 4, i_subplot)\n",
    "    i = np.random.randint(len(test_X))\n",
    "    plt.imshow(test_imgs[i].T, cmap='Greys', interpolation='none', origin='lower', \n",
    "               extent=[0, img_size, 0, img_size])\n",
    "    for pred_bbox, exp_bbox, pred_shape in zip(pred_bboxes[i], test_bboxes[i], pred_shapes[i]):\n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((pred_bbox[0], pred_bbox[1]), pred_bbox[2], pred_bbox[3], \n",
    "                                                         ec='r' if pred_shape[0] <= 0.5 else 'y', fc='none'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both bounding boxes seem to be in the middle of the rectangles. What happened? Imagine the following situation: We train our network on the leftmost image in the plot above. Let’s say that the expected bounding box of the left rectangle is at position 1 in the target vector (x1, y1, w1, h1), and the expected bounding box of the right rectangle is at position 2 in the vector (x2, y2, w2, h2). Apparently, our optimizer will change the parameters of the network so that the first predictor moves to the left, and the second predictor moves to the right. Imagine now that a bit later we come across a similar image, but this time the positions in the target vector are swapped (i.e. left rectangle at position 2, right rectangle at position 1). Now, our optimizer will pull predictor 1 to the right and predictor 2 to the left — exactly the opposite of the previous update step! In effect, the predicted bounding boxes stay in the center. And as we have a huge dataset (40k images), there will be quite a lot of such “duplicates”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to “assign” each predicted bounding box to a rectangle during training. Then, the predictors can learn to specialize on certain locations and/or shapes of rectangles. In order to do this, we process the target vectors after every epoch: For each training image, we calculate the mean squared error between the prediction and the target A) for the current order of bounding boxes in the target vector (i.e. x1, y1, w1, h1, x2, y2, w2, h2) and B) if the bounding boxes in the target vector are flipped (i.e. x2, y2, w2, h2, x1, y1, w1, h1). If the MSE of A is greater than B, we leave the target vector as is; if the MSE of B is greater than A, we flip the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write a function that permutes the ground truth labels based on the obtained mean square error. This function uses the library `munkres` to compute the minimum cost assignment given the matrix of costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from munkres import Munkres\n",
    "\n",
    "def flip_targets(pred_ys,ys,m):\n",
    "\n",
    "    for sample, (pred_y,y) in enumerate(zip(pred_ys,ys)):\n",
    "        pred_y = pred_y.reshape(num_objects, -1)\n",
    "        y = y.reshape(num_objects, -1)\n",
    "        \n",
    "        pred_bboxes = pred_y[:, :4]\n",
    "        y_bboxes = y[:, :4]\n",
    "        costs = np.zeros((num_objects,num_objects))\n",
    "        for i, y_bbox in enumerate(y_bboxes):\n",
    "            for j, pred_bbox in enumerate(pred_bboxes):\n",
    "                costs[i, j] += np.mean(np.square(y_bbox - pred_bbox))\n",
    "                \n",
    "        # matrix of assignments\n",
    "        idxs = np.array(m.compute(costs))[:,1]\n",
    "        y_flipped = y[idxs]\n",
    "        ys[sample] = y_flipped.flatten()\n",
    "        \n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Dense(256, input_dim=X.shape[-1]), \n",
    "        Activation('relu'), \n",
    "        Dropout(0.4), \n",
    "        Dense(y.shape[-1])\n",
    "    ])\n",
    "model.compile('adadelta', 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train with an external loop this time. This will allow us to flip the ground truth at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = Munkres()\n",
    "for epoch in range(n_epochs):\n",
    "    print ('Epoch', epoch)\n",
    "    model.fit(train_X,train_y,batch_size=512,epochs=1,validation_data=(val_X,val_y))\n",
    "    pred_y_train = model.predict(train_X)\n",
    "    pred_y_val = model.predict(val_X)\n",
    "    \n",
    "    train_y = flip_targets(pred_y_train,train_y,m)\n",
    "    val_y = flip_targets(pred_y_val,val_y,m)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's decode some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(test_X)\n",
    "pred_y = pred_y.reshape(len(pred_y), num_objects, -1)\n",
    "pred_bboxes = pred_y[..., :4] * img_size\n",
    "pred_shapes = pred_y[..., 4:5]\n",
    "pred_bboxes.shape, pred_shapes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "for i_subplot in range(1, 9):\n",
    "    plt.subplot(2, 4, i_subplot)\n",
    "    i = np.random.randint(len(test_X))\n",
    "    plt.imshow(test_imgs[i].T, cmap='Greys', interpolation='none', \n",
    "               origin='lower', extent=[0, img_size, 0, img_size])\n",
    "    for pred_bbox, exp_bbox, pred_shape in zip(pred_bboxes[i], test_bboxes[i], pred_shapes[i]):\n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((pred_bbox[0], pred_bbox[1]), pred_bbox[2], pred_bbox[3], \n",
    "                                                         ec='r' if pred_shape[0] <= 0.5 else 'y', fc='none'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It still is not perfect, but remember that we are using a very simple fully connected network ! You can try to change the model and see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get real..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recognizing shapes is a cool and easy example, but obviously it’s not what you want to do in the real world (there aren’t that many abstract 2D shapes in nature, unfortunately). Also, our algorithm can only predict a fixed number of bounding boxes per image. In the real world, however, you have diverse scenarios: A small side road may have no cars on it, but as soon as you drive on the highway, you have to recognize hundreds of cars at the same time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
