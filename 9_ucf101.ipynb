{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action classification in video: UCF-101\n",
    "\n",
    "We will design an algorithm for action classification on UCF-101, a dataset with >13k videos covering 101 action classes. The pipeline consists in extracting frame-level features using a pre-trained CNN and then modelling their temporal evolution using RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "from util.video_loader import generate_batch\n",
    "\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pattern = 'data/ucf101/*'\n",
    "num_classes = 101\n",
    "batch_size = 256\n",
    "cell_type = 'gru'  # 'lstm' or 'gru'\n",
    "rnn_layers = 2\n",
    "rnn_cells = 256\n",
    "learning_rate = 0.001\n",
    "grad_clip_norm = 1.\n",
    "num_iterations = 500\n",
    "print_freq = 10  # how often to print during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the videos: TFRecord\n",
    "\n",
    "Extracting CNN features for each frame in a video is computationally expensive. For this reason, we already extracted features for the first 10s of every video in UCF-101 (padding shorter videos with empty frames) we will focus on developing and training the RNN model. Since we do not need to allocate memory for the CNN, we can use larger batch sizes and iterate much faster over the data.\n",
    "\n",
    "The features are stored using TFRecord, a data format that enables fast and asynchronous data loading. This means that while the GPU is busy processing the current batch, the CPU will load and preprocess the next one; such procedure allows to make the most of the available hardware and avoid starving the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset graph in case we already created one and want to change hyperparameters\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create data loading pipeline\n",
    "model_input, labels, seq_length, _ = generate_batch(train_data_pattern, batch_size, train=True)\n",
    "\n",
    "# We will use one label for the whole sequence\n",
    "labels = labels[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN cells\n",
    "cell_fn = {'lstm': tf.contrib.rnn.LSTMCell, 'gru': tf.contrib.rnn.GRUCell}\n",
    "cell_list = [cell_fn[cell_type](rnn_cells) for _ in range(rnn_layers)]\n",
    "multi_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "\n",
    "# Unroll RNN dynamically\n",
    "rnn_outputs, rnn_states = tf.nn.dynamic_rnn(multi_cell, model_input, dtype=tf.float32, sequence_length=seq_length)\n",
    "\n",
    "# Fully-connected layer on top of the last RNN output\n",
    "logits = layers.linear(inputs=rnn_outputs[:, -1, :], num_outputs=num_classes + 1)  # we have the background class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cross-entropy loss\n",
    "cross_entropy_per_sample = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "loss = tf.reduce_mean(cross_entropy_per_sample)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), labels), tf.float32))\n",
    "\n",
    "# Define the backwards pass (gradients and parameter update)\n",
    "opt = tf.train.AdamOptimizer(tf.constant(learning_rate))\n",
    "vars_to_optimize = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, vars_to_optimize), clip_norm=grad_clip_norm)\n",
    "grads_and_vars = list(zip(grads, vars_to_optimize))\n",
    "train_fn = opt.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where the logs will be stored\n",
    "logdir = 'tensorboard_logs/ucf101'\n",
    "\n",
    "# Define individual summaries\n",
    "loss_summary = tf.summary.scalar(\"cross_entropy_loss\", loss)\n",
    "accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "summary_op = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "\n",
    "# Create an empty directory for the logs\n",
    "if tf.gfile.Exists(logdir):\n",
    "    tf.gfile.DeleteRecursively(logdir)\n",
    "if not tf.gfile.Exists(logdir):\n",
    "    tf.gfile.MakeDirs(logdir)\n",
    "    \n",
    "# Create the writer\n",
    "summary_writer = tf.summary.FileWriter(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session and initialize variables\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Initialize queue runners in the data loading pipeline\n",
    "# tf.train.start_queue_runners(sess=sess); TODO: check if this is needed\n",
    "coord = tf.train.Coordinator()\n",
    "threads = []\n",
    "for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n",
    "    threads.extend(qr.create_threads(sess, coord=coord, daemon=True, start=True))\n",
    "\n",
    "# Training loop\n",
    "try:\n",
    "    duration = 0.\n",
    "    for step in range(num_iterations):\n",
    "        start_time = time.time()\n",
    "        iter_cross_entropy, iter_acc, _, summary_str = sess.run([loss, accuracy, train_fn, summary_op])\n",
    "        duration += time.time() - start_time\n",
    "\n",
    "        # Write TensorBoard summaries and print training evolution every once in a while\n",
    "        if step > 0 and (step % print_freq == 0 or step == (num_iterations - 1)):\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            print(\"Step %d, \"\n",
    "                  \"%.1f examples/sec (%.2f sec/batch), \"\n",
    "                  \"loss: %.7f, \"\n",
    "                  \"accuracy: %.2f %%\" % (step,\n",
    "                                         print_freq * batch_size / duration,\n",
    "                                         duration / print_freq,\n",
    "                                         iter_cross_entropy,\n",
    "                                         100. * iter_acc))\n",
    "            duration = 0.\n",
    "    print('Done!')\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training was interrupted\")\n",
    "finally:\n",
    "    # Stop the writer\n",
    "    summary_writer.flush()\n",
    "    summary_writer.close()\n",
    "\n",
    "    # Stop data loading threads\n",
    "    coord.request_stop()\n",
    "    coord.join(threads, stop_grace_period_secs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible extensions\n",
    "\n",
    "- Consecutive frames in a video are highly redundant. Can we improve performance by decreasing the rate at which frames are sampled?\n",
    "- Propagating the gradients from the last time step only may become difficult for long sequences. How would you introduce supervision at frame-level?\n",
    "- The UCF-101 dataset is rather small. How would you reduce overfitting?\n",
    "- How would you load raw videos in TensorFlow to extract CNN features? Loading video files is not currently supported in TensorFlow (see this [issue on github](https://github.com/tensorflow/tensorflow/issues/6265#issuecomment-268338817) for more information). We used [our own custom python op](https://github.com/victorcampos7/tensorflow-ffmpeg) that wraps ffmpeg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
