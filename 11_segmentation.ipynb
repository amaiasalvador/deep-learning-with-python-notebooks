{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session we will see how to build a simple ConvNet for a toy semantic segmentation problem. We train the network from scratch and we will see the particularities of the problem. Finally, we will use a state of the art object detection network on some real-world images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deal with images with simple objects of different shapes and colors. Our goal is: given an image with objects of different shapes and colors, classify each pixel with a label corresponding the type of object it belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the dataset ourselves with the following code. Our dataset will be composed of images containing objects of three different shapes (triangles, circles and squares). These will be of different sizes, colours, and they might overlap. Our task is to predict the object shape for each pixel of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "palette = {(0,0,0):0, (0,0,255):1,(0,255,0):2,(255,0,0):3}\n",
    "def convert_from_color_segmentation(arr_3d, palette, image_height=32, image_width=32):\n",
    "\n",
    "    reshape_array = np.reshape(arr_3d, [image_height * image_width, 3])\n",
    "\n",
    "    #still too slow!!\n",
    "    arr_2d = np.fromiter([palette.get((x[0], x[1], x[2]), 0) for x in reshape_array],\n",
    "                         reshape_array.dtype)\n",
    "\n",
    "    return np.reshape(np.asarray(arr_2d), arr_3d.shape[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cairo\n",
    "num_imgs = 5000\n",
    "\n",
    "img_size = 32\n",
    "min_object_size = 4\n",
    "max_object_size = 16\n",
    "num_objects = 4\n",
    "\n",
    "shape_labels = ['rectangle', 'circle', 'triangle']\n",
    "num_shapes = len(shape_labels)\n",
    "\n",
    "imgs = np.zeros((num_imgs, img_size, img_size, 4), dtype=np.uint8)  # format: BGRA\n",
    "masks = np.zeros((num_imgs, img_size, img_size, 4), dtype=np.uint8)\n",
    "masks_decoded = []\n",
    "shapes = np.zeros((num_imgs, num_objects), dtype=int)\n",
    "colors = np.zeros((num_imgs, num_objects), dtype=int)\n",
    "\n",
    "colors = [[0,0,1],[0,1,0],[1,0,0],[1,0,1],[1,1,0]]\n",
    "num_colors = len(colors)\n",
    "\n",
    "for i_img in range(num_imgs):\n",
    "    surface = cairo.ImageSurface.create_for_data(imgs[i_img], cairo.FORMAT_ARGB32, img_size, img_size)\n",
    "    surface_mask = cairo.ImageSurface.create_for_data(masks[i_img], cairo.FORMAT_ARGB32, img_size, img_size)\n",
    "    \n",
    "    cr = cairo.Context(surface)\n",
    "    cr_mask = cairo.Context(surface_mask)\n",
    "    # Fill background white.\n",
    "    cr.set_source_rgb(1, 1, 1)\n",
    "    cr.paint()\n",
    "    \n",
    "    cr_mask.set_source_rgb(0,0,0)\n",
    "    cr_mask.paint()\n",
    "    \n",
    "    # Draw random shapes.\n",
    "    for i_object in range(num_objects):\n",
    "        shape = np.random.randint(num_shapes)\n",
    "        shapes[i_img, i_object] = shape\n",
    "        if shape == 0:  # rectangle\n",
    "            w, h = np.random.randint(min_object_size, max_object_size, size=2)\n",
    "            x = np.random.randint(0, img_size - w)\n",
    "            y = np.random.randint(0, img_size - h)\n",
    "            cr.rectangle(x, y, w, h)\n",
    "            cr_mask.rectangle(x, y, w, h)\n",
    "            cr_mask.set_source_rgb(0,0,1)\n",
    "            cr_mask.fill()\n",
    "        elif shape == 1:  # circle   \n",
    "            r = 0.5 * np.random.randint(min_object_size, max_object_size)\n",
    "            x = np.random.randint(r, img_size - r)\n",
    "            y = np.random.randint(r, img_size - r)\n",
    "            cr.arc(x, y, r, 0, 2*np.pi)\n",
    "            cr_mask.arc(x, y, r, 0, 2*np.pi)\n",
    "            cr_mask.set_source_rgb(0,1,0)\n",
    "            cr_mask.fill()\n",
    "        elif shape == 2:  # triangle\n",
    "            w, h = np.random.randint(min_object_size, max_object_size, size=2)\n",
    "            x = np.random.randint(0, img_size - w)\n",
    "            y = np.random.randint(0, img_size - h)\n",
    "            cr.move_to(x, y)\n",
    "            cr.line_to(x+w, y)\n",
    "            cr.line_to(x+w, y+h)\n",
    "            cr.line_to(x, y)\n",
    "            cr.close_path()\n",
    "            \n",
    "            cr_mask.move_to(x, y)\n",
    "            cr_mask.line_to(x+w, y)\n",
    "            cr_mask.line_to(x+w, y+h)\n",
    "            cr_mask.line_to(x, y)\n",
    "            cr_mask.close_path()\n",
    "            \n",
    "            cr_mask.set_source_rgb(1,0,0)\n",
    "            cr_mask.fill()\n",
    "        \n",
    "        # TODO: Introduce some variation to the colors by adding a small random offset to the rgb values.\n",
    "        color = np.random.randint(num_colors)\n",
    "        r,g,b = colors[color]\n",
    "        max_offset = 0.3\n",
    "        r_offset, g_offset, b_offset = max_offset * 2. * (np.random.rand(3) - 0.5)\n",
    "        cr.set_source_rgb(r-max_offset+r_offset, g+g_offset, b+b_offset)\n",
    "        cr.fill()\n",
    "    masks_decoded.append(convert_from_color_segmentation(masks[i_img][:,:,0:3],palette))\n",
    "        \n",
    "imgs = imgs[..., 2::-1]\n",
    "masks_decoded = np.array(masks_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_decoded.shape, imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of the samples we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masks_decoded[2])\n",
    "plt.show()\n",
    "plt.imshow(imgs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "masks_decoded = masks_decoded.reshape(-1,1)\n",
    "masks_decoded_cat = to_categorical(masks_decoded,num_classes = len(shape_labels) + 1)\n",
    "masks_decoded_cat = masks_decoded_cat.reshape(num_imgs,img_size,img_size,len(shape_labels) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (imgs - np.mean(imgs)) / np.std(imgs)\n",
    "X.shape, np.mean(X), np.std(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_train = int(0.6 * num_imgs)\n",
    "i_val = int(0.7 * num_imgs)\n",
    "\n",
    "train_X = X[:i_train]\n",
    "val_X = X[i_train:i_val]\n",
    "test_X = X[i_val:]\n",
    "train_y = masks_decoded_cat[:i_train]\n",
    "val_y = masks_decoded_cat[i_train:i_val]\n",
    "test_y = masks_decoded_cat[i_val:]\n",
    "test_imgs = imgs[i_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a simple model composed of 4 convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Conv2D(filters=64,kernel_size=3, input_shape=(X.shape[1:]),activation='relu',padding='same'), \n",
    "        Conv2D(filters=64,kernel_size=3,activation='relu', padding='same'),\n",
    "        Conv2D(filters=32,kernel_size=3,activation='relu', padding='same'),\n",
    "        Conv2D(filters=num_shapes+1,kernel_size=3,activation='softmax',padding='same')\n",
    "    ])\n",
    "model.compile('adadelta', 'categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "history = model.fit(train_X,train_y,batch_size=512,epochs=n_epochs,validation_data=(val_X,val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triangle:1, Circle:2, Square:3\n",
    "\n",
    "for i,pred in enumerate(preds[0:5]):\n",
    "    fig, (ax0, ax1,ax2) = plt.subplots(ncols=3,figsize=(30,10))\n",
    "    argmax_pred = np.argmax(pred,axis=-1)\n",
    "    cf0 = ax0.imshow(test_imgs[i])\n",
    "    fig.colorbar(cf0,ax=ax0)\n",
    "    cf1 = ax1.imshow(argmax_pred,vmin=0,vmax=3,cmap='magma')\n",
    "    fig.colorbar(cf1,ax=ax1)\n",
    "    cf2 = ax2.imshow(np.argmax(test_y[i],axis=-1),vmin=0,vmax=3,cmap='magma')\n",
    "    fig.colorbar(cf2,ax=ax2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Segmentation of Real-World Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's dive into a network that can deal with real images !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a pretrained semantic segmentation network to obtain results on some of our test images. The network we chose is the one introduced in:\n",
    "\n",
    "Jonathan Long, Evan Shelhamer, Trevor Darrell. [Fully Convolutional Networks for Semantic Segmentation](www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf\n",
    "). CVPR 2015\n",
    "\n",
    "\n",
    "FCN is composed of a ConvNet from which final classifier layer has been removed and all fully connected layers have been converted to convolutions. We append a 1x1 convolution with channel dimension 21 to predict scores for each of the PASCAL classes (including background) at each of the coarse output locations, followed by a deconvolution layer to bilinearly upsample the coarse outputs to pixel-dense outputs.\n",
    "\n",
    "FCN combines layers of the feature hierarchy and refines the spatial precision of the output. While fully convolutionalized classifiers can be fine-tuned to segmentation, and even score highly on the standard metric, their output is dissatisfyingly coarse. The 32 pixel stride at the final prediction layer limits the scale of detail in the upsampled output.\n",
    "This issue is addressed by adding skips that combine the final prediction layer with lower layers with finer strides. As they see fewer pixels, the finer scale predictions should need fewer layers, so it makes sense to make them from shallower net outputs. Combining fine layers and coarse layers lets the model make local predictions that respect global structure.\n",
    "We first divide the output stride in half by predicting from a 16 pixel stride layer. We add a 1x1 convolution layer on top of pool4 to produce additional class predictions. We fuse this output with the predictions computed on top of conv7 (convolutionalized fc7) at stride 32 by adding a 2x upsampling layer and summing both predictions. Finally, the stride 16 predictions are upsampled back to the image.\n",
    "\n",
    "We call this net FCN-16s.\n",
    "\n",
    "Remark :\n",
    "The original paper mention that FCN-8s (slightly more complex architecture) does not provide much improvement so we stopped at FCN-16s\n",
    "\n",
    "Note: the code is adapted from [this notebook](https://github.com/mzaradzki/neuralnets/blob/master/vgg_segmentation_keras/fcn16s_segmentation_keras2.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'util/fcn/')\n",
    "from util.fcn.fcn import fcn32_blank, fcn_32s_to_16s, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 64*8 # INFO: initially tested with 256, 448, 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn32model = fcn32_blank(image_size)\n",
    "fcn16model = fcn_32s_to_16s(fcn32model)\n",
    "fcn16model.summary() # visual inspection of model architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load the pretrained weights to the model we created. The model was trained with matconvnet, so we need to load the `.mat` file and decode the weights so that our keras model can understand them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "data = loadmat('data/models/pascal-fcn16s-dag.mat', matlab_compatible=False, struct_as_record=False)\n",
    "l = data['layers']\n",
    "p = data['params']\n",
    "description = data['meta'][0,0].classes[0,0].description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `.mat` file stores information of the layers, the parameters (pooling layers not included here) and the output categories of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.shape, p.shape, description.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2index = {}\n",
    "for i, clname in enumerate(description[0,:]):\n",
    "    class2index[str(clname[0])] = i\n",
    "    \n",
    "print(sorted(class2index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, p.shape[1]-1, 2):\n",
    "    print(i,\n",
    "          str(p[0,i].name[0]), p[0,i].value.shape,\n",
    "          str(p[0,i+1].name[0]), p[0,i+1].value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the function that we will use to copy the weights from the `.mat` file to the keras model. We basically will check that the layer names & layer shapes match, and will set the weights and biases with `model.layers[idx].set_weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_mat_to_keras(kmodel):\n",
    "    \n",
    "    kerasnames = [lr.name for lr in kmodel.layers]\n",
    "\n",
    "    prmt = (0, 1, 2, 3) # WARNING : important setting as 2 of the 4 axis have same size dimension\n",
    "    \n",
    "    for i in range(0, p.shape[1]-1, 2):\n",
    "        matname = '_'.join(p[0,i].name[0].split('_')[0:-1])\n",
    "        if matname in kerasnames:\n",
    "            kindex = kerasnames.index(matname)\n",
    "            print('found : ', (str(matname), kindex))\n",
    "            l_weights = p[0,i].value\n",
    "            l_bias = p[0,i+1].value\n",
    "            f_l_weights = l_weights.transpose(prmt)\n",
    "            if False: # WARNING : this depends on \"image_data_format\":\"channels_last\" in keras.json file\n",
    "                f_l_weights = np.flip(f_l_weights, 0)\n",
    "                f_l_weights = np.flip(f_l_weights, 1)\n",
    "            print(f_l_weights.shape, kmodel.layers[kindex].get_weights()[0].shape)\n",
    "            assert (f_l_weights.shape == kmodel.layers[kindex].get_weights()[0].shape)\n",
    "            assert (l_bias.shape[1] == 1)\n",
    "            assert (l_bias[:,0].shape == kmodel.layers[kindex].get_weights()[1].shape)\n",
    "            assert (len(kmodel.layers[kindex].get_weights()) == 2)\n",
    "            kmodel.layers[kindex].set_weights([f_l_weights, l_bias[:,0]])\n",
    "        else:\n",
    "            print('not found : ', str(matname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_mat_to_keras(fcn16model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to forward an image through the network and get some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "im = Image.open('data/images/bus_creative_commons.jpg')\n",
    "im = im.resize((image_size,image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.asarray(im))\n",
    "print(np.asarray(im).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = prediction(fcn16model, im, transform=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will display the class with maximum probability for each pixel in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds.shape)\n",
    "imclass = np.argmax(preds, axis=3)[0,:,:]\n",
    "print(imclass.shape)\n",
    "plt.figure(figsize = (15, 7))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow( np.asarray(im) )\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow( imclass )\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow( np.asarray(im) )\n",
    "masked_imclass = np.ma.masked_where(imclass == 0, imclass)\n",
    "#plt.imshow( imclass, alpha=0.5 )\n",
    "plt.imshow( masked_imclass, alpha=0.5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the list of classes that were found in the image (those that obtained the maximum probability in at least one of the pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in np.unique(imclass):\n",
    "    print(c, str(description[0,c][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will pick some of the classes and display its activation values for all pixels in the image. Here we are not displaying the class with maximum value for each pixel, but the actual value for all pixels given a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import bytescale\n",
    "bspreds = bytescale(preds, low=0, high=255)\n",
    "print (bspreds.shape)\n",
    "plt.figure(figsize = (15, 7))\n",
    "plt.subplot(2,3,1)\n",
    "plt.imshow(np.asarray(im))\n",
    "plt.subplot(2,3,3+1)\n",
    "plt.imshow(bspreds[0,:,:,class2index['background']], cmap='seismic')\n",
    "plt.subplot(2,3,3+2)\n",
    "plt.imshow(bspreds[0,:,:,class2index['bus']], cmap='seismic')\n",
    "plt.subplot(2,3,3+3)\n",
    "plt.imshow(bspreds[0,:,:,class2index['person']], cmap='seismic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the segmentation of the person in the image being much more accurate here. However, the probability of the category `bus` appeared to be higher in most pixels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
